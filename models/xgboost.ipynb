{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "import mlflow\n",
    "import boto3\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#load data\n",
    "sample = pd.read_csv('../data/sample_submission.csv')\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "#ProfileReport(train, title=\"Profiling Report\")\n",
    "\n",
    "#drop some vars\n",
    "drop_list = ['PassengerId', 'Name', 'Cabin']\n",
    "train = train.drop(drop_list, axis=1)\n",
    "train = train.dropna(how='any')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3クライアントの初期化\n",
    "BUCKET_NAME = 'your-buket-name'\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# ファイルをローカルにダウンロード\n",
    "s3_client.download_file(BUCKET_NAME, 'data/train.csv', 'train.csv')\n",
    "\n",
    "# Pandasを使用してCSVファイルを読み込む\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# データフレームの内容を表示\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バケット内のオブジェクトのリストを取得\n",
    "objects = s3_client.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "\n",
    "# バケット内のオブジェクトのキーを表示\n",
    "if 'Contents' in objects:\n",
    "    for obj in objects['Contents']:\n",
    "        print(obj['Key'])\n",
    "else:\n",
    "    print(\"バケット内にオブジェクトが存在しません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_list = ['HomePlanet', 'Destination', 'CryoSleep', 'VIP', 'Transported']\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# インスタンス化\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "for column in onehot_list:\n",
    "    # OneHotエンコーディングを適用\n",
    "    transformed = enc.fit_transform(train[[column]])\n",
    "    \n",
    "    # エンコーディングされたデータをDataFrameに変換\n",
    "    transformed_df = pd.DataFrame(transformed, columns=[f\"{column}_{cat}\" for cat in enc.categories_[0]], index=train.index)  # インデックスを指定\n",
    "    \n",
    "    # 元のデータから対象の列を削除\n",
    "    train = train.drop(column, axis=1)\n",
    "    \n",
    "    # エンコーディングされたデータを元のDataFrameに結合\n",
    "    train = pd.concat([train, transformed_df], axis=1)\n",
    "\n",
    "train = train.drop(['HomePlanet_Mars', 'Destination_TRAPPIST-1e', 'CryoSleep_False', 'VIP_False', 'Transported_False'], axis=1)\n",
    "train.head()\n",
    "\n",
    "#説明変数と被説明変数に分割\n",
    "x = train.drop(['Transported_True'], axis=1)\n",
    "y = train['Transported_True']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "train_all = pd.concat([y_train, X_train], axis=1)\n",
    "\n",
    "\n",
    "#cross varidation\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "params = {'max_depth':3, 'eta':0.1}\n",
    "cross_val = xgb.cv(\n",
    "    params, dtrain, num_boost_round=1000, early_stopping_rounds=50\n",
    ")\n",
    "best_n_boost_round = cross_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(cross_val.loc[:, ['test-rmse-mean', 'train-rmse-mean']])\n",
    "plt.grid()\n",
    "plt.xlabel('num_boost_round')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    'n_estimators',  # Number of gradient boosted trees. Equivalent to number of boosting rounds.\n",
    "    'max_depth',  # Maximum tree depth for base learners.\n",
    "    'max_leaves',  # Maximum number of leaves; 0 indicates no limit.\n",
    "    'max_bin',  # If using histogram-based algorithm, maximum number of bins per feature.\n",
    "    'grow_policy',  # Tree growing policy.\n",
    "    'learning_rate',  # Boosting learning rate (xgb's \"eta\").\n",
    "    'verbosity',  # The degree of verbosity.\n",
    "    'objective',  # Specify the learning task and the corresponding learning objective or a custom objective function.\n",
    "    'booster',  # Specify which booster to use: gbtree, gblinear or dart.\n",
    "    'tree_method',  # Specify which tree method to use.\n",
    "    'n_jobs',  # Number of parallel threads used to run xgboost.\n",
    "    'gamma',  # Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "    'min_child_weight',  # Minimum sum of instance weight(hessian) needed in a child.\n",
    "    'max_delta_step',  # Maximum delta step we allow each tree's weight estimation to be.\n",
    "    'subsample',  # Subsample ratio of the training instance.\n",
    "    'sampling_method',  # Sampling method. Used only by the GPU version of \"hist\" tree method.\n",
    "    'colsample_bytree',  # Subsample ratio of columns when constructing each tree.\n",
    "    'colsample_bylevel',  # Subsample ratio of columns for each level.\n",
    "    'colsample_bynode',  # Subsample ratio of columns for each split.\n",
    "    'reg_alpha',  # L1 regularization term on weights (xgb's alpha). LASSO\n",
    "    'reg_lambda',  # L2 regularization term on weights (xgb's lambda). Ridge\n",
    "    'scale_pos_weight',  # Balancing of positive and negative weights.\n",
    "    'base_score',  # The initial prediction score of all instances, global bias.\n",
    "    'random_state',  # Random number seed.\n",
    "    'missing',  # Value in the data which needs to be present as a missing value.\n",
    "    'num_parallel_tree',  # Used for boosting random forest.\n",
    "    'monotone_constraints',  # Constraint of variable monotonicity.\n",
    "    'interaction_constraints',  # Constraints for interaction representing permitted interactions.\n",
    "    'importance_type',  # The feature importance type for the feature_importances_ property.\n",
    "    'device',  # Device ordinal, available options are \"cpu\", \"cuda\", and \"gpu\".\n",
    "    'validate_parameters',  # Give warnings for unknown parameter.\n",
    "    'enable_categorical',  # Experimental support for categorical data.\n",
    "    'feature_types',  # Used for specifying feature types without constructing a dataframe.\n",
    "    'max_cat_to_onehot',  # A threshold for deciding whether to use one-hot encoding based split for categorical data.\n",
    "    'max_cat_threshold',  # Maximum number of categories considered for each split.\n",
    "    'multi_strategy',  # The strategy used for training multi-target models.\n",
    "    'eval_metric',  # Metric used for monitoring the training result and early stopping.\n",
    "    'early_stopping_rounds',  # Activates early stopping.\n",
    "    'callbacks',  # List of callback functions that are applied at end of each iteration.\n",
    "    'kwargs'  # Keyword arguments for XGBoost Booster object.\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベイズ最適化による最適ハイパーパラメータ詮索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラウド上で動かしているmlflow tracking serverのパブリックURL\n",
    "TRACKING_SERVER_HOST = \"your-mlflow-tracking-server-host\"\n",
    "\n",
    "# URLをセット\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:5000\") \n",
    "print(f\"Tracking Server URI: '{mlflow.get_tracking_uri()}'\")\n",
    "\n",
    "#experimentの名前を設定(今回は日付け情報)\n",
    "dt = datetime.datetime.today()\n",
    "exp_name = f'exp_{dt.date()}'\n",
    "#experimentを作成\n",
    "experiment = mlflow.create_experiment(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre setting of categorical parameters\n",
    "try_grow_policy = 'depthwise'\n",
    "try_objective = 'reg:squarederror'\n",
    "try_booster = 'gbtree'\n",
    "try_tree_method = 'auto'\n",
    "try_sampling_method = 'uniform'\n",
    "try_importance_type = 'gain'\n",
    "try_device = 'cpu'\n",
    "try_multi_strategy = 'diagonal'\n",
    "try_eval_metric = 'rmse'\n",
    "\n",
    "#evaluation function\n",
    "def xgboost_eval(try_max_depth,\n",
    "                 try_learning_rate, \n",
    "                 try_n_estimators, \n",
    "                 try_gamma, \n",
    "                 try_min_child_weight, \n",
    "                 try_subsample, \n",
    "                 try_colsample_bytree, \n",
    "                 try_reg_alpha, \n",
    "                 try_reg_lambda):\n",
    "    # convert to int since these are not continuous variables\n",
    "    try_max_depth = int(try_max_depth)\n",
    "    try_n_estimators = int(try_n_estimators)\n",
    "\n",
    "    #parameter settings\n",
    "    #最大値を設定する系はナシ\n",
    "    #データセットの前処理に関わる変数もナシ。\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=try_max_depth,\n",
    "        learning_rate=try_learning_rate,\n",
    "        n_estimators=try_n_estimators,\n",
    "        gamma=try_gamma,\n",
    "        min_child_weight=try_min_child_weight,\n",
    "        subsample=try_subsample,\n",
    "        colsample_bytree=try_colsample_bytree,\n",
    "        reg_alpha=try_reg_alpha,\n",
    "        reg_lambda=try_reg_lambda,\n",
    "        try_grow_policy = try_grow_policy,\n",
    "        try_objctive = try_objective,\n",
    "        try_booster = try_booster,\n",
    "        try_tree_method = try_tree_method,\n",
    "        try_importance_type = try_importance_type,\n",
    "        try_device = try_device,\n",
    "        try_multi_strategy = try_multi_strategy,\n",
    "        try_eval_metric = try_eval_metric\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # model training\n",
    "    model.fit(X_train, y_train)\n",
    "    # calculate model score\n",
    "    score = model.score(X_test, y_test)\n",
    "    #start logging (nested)\n",
    "    with mlflow.start_run(run_name = 'XGBoost',\n",
    "                          experiment_id= experiment,\n",
    "                          nested = True):\n",
    "        #logging settings\n",
    "        mlflow.log_params({\n",
    "            'max_depth': try_max_depth,\n",
    "            'learning_rate': try_learning_rate,\n",
    "            'n_estimators': try_n_estimators,\n",
    "            'gamma': try_gamma,\n",
    "            'min_child_weight': try_min_child_weight,\n",
    "            'subsample': try_subsample,\n",
    "            'colsample_bytree': try_colsample_bytree,\n",
    "            'reg_alpha': try_reg_alpha,\n",
    "            'reg_lambda': try_reg_lambda,\n",
    "            'grow_policy': try_grow_policy,\n",
    "            'objective': try_objective,\n",
    "            'booster': try_booster,\n",
    "            'tree_method': try_tree_method,\n",
    "            'sampling_method': try_sampling_method,\n",
    "            'importance_type': try_importance_type,\n",
    "            'device': try_device,\n",
    "            'multi_strategy': try_multi_strategy,\n",
    "            'eval_metric': try_eval_metric,\n",
    "            'score': score\n",
    "            })\n",
    "        mlflow.xgboost.log_model(model,'mdoel')\n",
    "        \n",
    "    return score\n",
    "\n",
    "# set search bounds of each parameter\n",
    "pbounds = {\n",
    "    'try_max_depth': (3,5,10,50),\n",
    "    'try_learning_rate': (0.01,0.05,0.1,0.5),\n",
    "    'try_n_estimators': (100,500,1000),\n",
    "    'try_gamma': (0, 5),\n",
    "    'try_min_child_weight': (1,3,5,10),\n",
    "    'try_subsample': (0.5, 1.0),\n",
    "    'try_colsample_bytree': (0.5, 1.0),\n",
    "    'try_reg_alpha': (0, 1),\n",
    "    'try_reg_lambda': (0, 1)\n",
    "}\n",
    "\n",
    "#start run experiment\n",
    "with mlflow.start_run(run_name='XGboost',\n",
    "                      experiment_id=experiment):\n",
    "    \n",
    "    #instansation of optimizer\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=xgboost_eval,\n",
    "        pbounds=pbounds,\n",
    "        random_state=1\n",
    "    )\n",
    "    \n",
    "    #calculation\n",
    "    optimizer.maximize(init_points=5, n_iter=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
